n_layers: 2                     # (int) The number of transformer layers in transformer encoder.
n_heads: 2                      # (int) The number of attention heads for multi-head attention layer.
hidden_size: 768                # (int) The number of features in the hidden state.
inner_size: 256                 # (int) The inner hidden size in feed-forward layer.
hidden_dropout_prob: 0.5        # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.5          # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'              # (str) The activation function in feed-forward layer.
layer_norm_eps: 1e-12           # (float) A value added to the denominator for numerical stability. 
initializer_range: 0.02         # (float) The standard deviation for normal initialization.
loss_type: 'CE'                 
seed: 2025


consistency_ratio: 0.5             
consistency_loss_weight: 0.1             

use_temporal_weight: True       
temporal_decay_rate: 0.5        
use_enhanced_position: True     
order_aware_loss_weight: 0.5   

################################################################
gpu_id: '1'
log_wandb: False

dataset: All_Beauty


MAX_ITEM_LIST_LENGTH: 50

USER_ID_FIELD: user_id
ITEM_ID_FIELD: item_id
load_col:
    inter: [user_id, item_id, timestamp]
user_inter_num_interval: "[1,inf)"    
item_inter_num_interval: "[1,inf)"
epochs: 400
train_batch_size: 512    
eval_batch_size: 2048
weight_decay: 0.0
learner: adam
learning_rate: 0.0001   
eval_step: 1
stopping_step: 5 
train_neg_sample_args: ~

metrics: ['Hit', 'NDCG', 'MRR', 'Recall', 'GAUC']
valid_metric: NDCG@20 

topk: [10,20]
